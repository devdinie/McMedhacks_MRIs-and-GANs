{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Part 2 - MNIST GAN Assignment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"tOmbK8YhsbiE"},"source":["# Generative adversarial network (GAN) for [MNIST](http://yann.lecun.com/exdb/mnist/) handwritten digit dataset\n","\n","During the workshop, you were shown how to implement a previously trained GAN. The task of building and training your own neural network that generates MRI data is an arduous one, often worthy of a PhD.\\\n","\\\n","Today, you will be given a few tasks that will allow you to examine a more basic GAN implementation found [here](https://github.com/lyeoni/pytorch-mnist-GAN/blob/master/pytorch-mnist-GAN.ipynb), that uses PyTorch and the MNIST handwritten digit dataset (you can find another, almost identical, script to do this [here](https://github.com/jsuarez5341/GAN-MNIST-Pytorch/blob/master/main.py)).\\\n","\\\n","This model is a lot simpler than what you would need to generate high-resolution and 3D MR images, but it will allow you to gain a better intuition for the architecture of GANs. Have fun filling in the blanks! :)"]},{"cell_type":"markdown","metadata":{"id":"agvCXYN9rPvc"},"source":["# Part 1: fill in the missing code"]},{"cell_type":"code","metadata":{"id":"g5WO1VbLS3QL"},"source":["# prerequisites\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","from torchvision.utils import save_image\n","import matplotlib.pyplot as plt\n","import os\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QCZpo94KS2Zq"},"source":["BATCH_SIZE = 100\n","\n","transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n","\n","# MNIST Dataset\n","train_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transform, download=True)\n","test_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transform, download=False)\n","\n","# Data Loader (Input Pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","\n","print(f\"You have {len(train_loader)} training instances and {len(test_loader)} testing instances.\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cHd10LQntXcv"},"source":["Take a quick look at your data"]},{"cell_type":"code","metadata":{"id":"1GHmnU0juIgi"},"source":["it=iter(train_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNuSJd4GuJ8k"},"source":["dataiter = iter(train_loader)\n","for i in range(3):\n","    images, labels = dataiter.next()\n","    print(images.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44xkqbYru9Ly"},"source":["**Questions:** \\\n","(a) What information does each of the tensors above give you?\\\n"," (b) Are these images in grayscale or in color? \\\n"," (c) How many channels are there in these images? \\"]},{"cell_type":"markdown","metadata":{"id":"icC1-F13bEGi"},"source":["Fill in the missing lines of code below (they start with ```# =======```)"]},{"cell_type":"code","metadata":{"id":"kjDfJZInbCns"},"source":["class Generator(nn.Module):\n","    def __init__(self, g_input_dim, g_output_dim):\n","        super(Generator, self).__init__()       \n","        self.fc1 = nn.Linear(g_input_dim, 256)\n","        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n","# ============ Add a 3rd linear layer here\n","        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n","    \n","    # forward method\n","    def forward(self, x): \n","        x = F.leaky_relu(self.fc1(x), 0.2)\n","# ============ Pass through your 2nd linear layer, then through a Leaky Relu function (slope=0.2)\n","# ============ Pass through your 3rd linear layer, then through a Leaky Relu function (slope=0.2)\n","        return torch.tanh(self.fc4(x))\n","    \n","class Discriminator(nn.Module):\n","    def __init__(self, d_input_dim):\n","        super(Discriminator, self).__init__()\n","        self.fc1 = nn.Linear(d_input_dim, 1024)\n","        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n","        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n","# ============ add a 4th linear layer that outputs 1 feature\n","    \n","    # forward method\n","    def forward(self, x):\n","        x = F.leaky_relu(self.fc1(x), 0.2)\n","        x = F.dropout(x, 0.3)\n","# ============ Pass through your 2nd linear layer, then through a Leaky Relu function (slope=0.2)\n","# ============ Add a dropout layer that zeros input elemnts with probability 0.3\n","# ============ Pass through your 3rd linear layer, then through a Leaky Relu function (slope=0.2)\n","# ============ Add a dropout layer that zeros input elemnts with probability 0.3\n","# ============ Pass your output through your 4th linear layer, then a sigmoid function, and return the result!"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u4H3y_PqbEb5"},"source":["# build network\n","z_dim = 100\n","mnist_dim = train_dataset.train_data.size(1) * train_dataset.train_data.size(2)\n","\n","G = Generator(g_input_dim = z_dim, g_output_dim = mnist_dim).to(device)\n","D = Discriminator(mnist_dim).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Tw0vhwcbJTd"},"source":["G"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gmkwzFO1bKOK"},"source":["D"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUZ1cTsObLLg"},"source":["# loss\n","criterion = nn.BCELoss() \n","\n","# optimizer\n","lr = 0.0002 \n","G_optimizer = optim.Adam(G.parameters(), lr = lr)\n","# ============ Define your discriminator Adam optimizer and call it D_optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DGDBtRKqbM33"},"source":["def D_train(x):\n","    ############### Train the discriminator ###############\n","    D.zero_grad()\n","\n","    # train discriminator on real\n","    x_real, y_real = x.view(-1, mnist_dim), torch.ones(BATCH_SIZE, 1)\n","    x_real, y_real = Variable(x_real.to(device)), Variable(y_real.to(device))\n","\n","    D_output = D(x_real)\n","    D_real_loss = criterion(D_output, y_real)\n","    D_real_score = D_output\n","\n","    # train discriminator on fake\n","    z = Variable(torch.randn(BATCH_SIZE, z_dim).to(device))\n","    x_fake, y_fake = G(z), Variable(torch.zeros(BATCH_SIZE, 1).to(device))\n","\n","    D_output = D(x_fake)\n","    D_fake_loss = criterion(D_output, y_fake)\n","    D_fake_score = D_output\n","\n","    # gradient backprop & optimize ONLY D's parameters\n","    D_loss = D_real_loss + D_fake_loss\n","    D_loss.backward()\n","    D_optimizer.step()\n","        \n","    return  D_loss.data.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JbEP6Y7tbPUF"},"source":["def G_train(x):\n","    ############### Train the generator ###############\n","# ============ Reset your generator parameter gradient to zero\n","\n","    z = Variable(torch.randn(BATCH_SIZE, z_dim).to(device))\n","    y = Variable(torch.ones(BATCH_SIZE, 1).to(device))\n","\n","    G_output = G(z)\n","    D_output = D(G_output)\n","    G_loss = criterion(D_output, y)\n","\n","    # gradient backprop & optimize ONLY G's parameters\n","    G_loss.backward()\n","# ============ Update your generator parameters\n","        \n","    return G_loss.data.item()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VqNLhrE3bRC6"},"source":["N_EPOCH = 100\n","for epoch in range(1, N_EPOCH+1):           \n","    D_losses, G_losses = [], []\n","    for batch_idx, (x, _) in enumerate(train_loader):\n","        D_losses.append(D_train(x))\n","        G_losses.append(G_train(x))\n","\n","    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n","            (epoch), N_EPOCH, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bIDVhCcrp61T"},"source":["os.mkdir(\"generated_digits_part_1\") # directory where you will save a tile of your fake images\n","with torch.no_grad():\n","    test_z = Variable(torch.randn(BATCH_SIZE, z_dim).to(device))\n","    generated = G(test_z)\n","    save_image(generated.view(generated.size(0), 1, 28, 28), f'generated_digits_part_1/sample_{BATCH_SIZE}batchSize{BATCH_SIZE}_{N_EPOCH}epochs.png')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aOsqb-1SFM6z"},"source":["import glob\n","\n","# take a look at your fake images\n","from IPython.display import Image\n","\n","for im in glob.glob('generated_digits_part_1/*.png'):\n","    print(f\"{im} :\")\n","    display(Image(im))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ey-qcNakrShr"},"source":["# Part 2: testing different batch sizes and numbers of epochs\n","Convert the code above into two nested loops that iterate through a list of batch sizes and epochs. Make sure to save your generated images with a meaningful naming convention (see above example). \\\n","**Tip:** We recommend nesting your N_EPOCH loop within the code where it was defined above, not to have to reload the data with every iteration. You will however have to reload the data for each change in batch size."]},{"cell_type":"code","metadata":{"id":"mP7MHjrXHnEF"},"source":["# directory where you will store your outputs:\n","os.mkdir(\"generated_digits_part_2\")"],"execution_count":null,"outputs":[]}]}
